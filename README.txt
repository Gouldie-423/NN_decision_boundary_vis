The purpose of this project was to iullistrate the learning process of a feed forward neural network with planar data. The final output NN_Visualization.gif showcases two groups (red & blue) randomly being generated into the planar space in two separate distributions. The light pink background color represents what the algorithm predicts as red if all of the remaining blank space was predicted by the network and vice versa with the light blue color. Each frame in the .gif is a new iteration of the network and it's accompanying predictions. The .gif is 30 frames long with corresponds with the 30 iterations the network was run for. The purpose of this project was to visualize the learning process for a feed forward neural network and the non-linearity that deep learning excels at relative to other ML models. If you look at NN_Visualization.gif the intermediate frames (epochs 15-25) the algorithm has a largely non-linear decision boundary before settling slightly into a non-linear decision boundary with a slight curve. This ability to have a non-linear decision boundary is part of what allows deep learning networks to understand the intricacies within their training data to a much greater extent than other traditional ML algorithims such as SVM.

This project takes place in two seperate files feed_forward_nn.py contains the network class. Within that class are 8 seperate functions. 

-init_params: because the data used in this project is so simple, a random initialization was used to populate the weights and biases. If there had been additional trouble learning xavier or glorot initialization would have been experimented with next. This function uses an explicit for loop that cycles over the layers_dims argument which contains the number of features, followed by the number of neurons at each layer in the network to create w&b of the correct size.

-linear_forward: contains the basic linear equation of Z=W*A_prev+b followed by a specific activation depending on where in the network this equation is being implemented. This is the basic building block that allows the network to propogate forward. This is also part of a vectorized implementation hence the lack of an explicit for loop for each training sample. 

-forward_pass: loops through each layer in the network and runs the linear activation for each one saving the output from each layer to pass it in as in input in the next layer. I use the tanh activation function in the intermediate layers because the function is centered around zero which in turn makes it easier for future layers to interpret the last activation. On the last layer we use the sigmoid output to get a float value between 0 and 1 that a decision boundary will later determine whether it is red or blue. This prediction will later be compared to the original labels via binary cross entropy loss to determine how much of a cost there is with the current network before updating weights & biases accordingly. We also use the forward pass function to make predictions based off of the 'blank_spots' tensor that we created earlier. 

-grad_sum: This is a function I'll use whenever I use tensorboard. It helps me determine whether or not I'm running into an exploding gradients problem which slows down if not stopping the learning process completely. By taking the absolute value of each weight/bias and summing them all together I get a rough idea of the collective distance from zero for each parameter. 

-graph_epoch_results: This function is used to help construct the final .gif. It identifies a decision boundary each iteration, which in reality is not a particularly good data science practice but it allows for us to have a perfect 50/50 split of red predicted & blue predicted points for the sake of visualizing a decision boundary. 

-run: this function puts all of the previous parts together. Beginning with the init_params function it randomly initializes all of the weights & biases required and defining our loss equation (Binary Cross Entropy) and our optimizer (Stochastic Gradient Descent). Next an explicit for loop for each epoch makes a prediction based off of training data and blank spot data before generating a plot visualizing them. Next the cost function compares the training data to its original labels before running backward propogation to generate gradients (or the derivative of each weight/bias with respect to the cost function) for each weight and bias. Next the optimizer updates the weights & biases incrimentally before zeroing out the gradients so the whole process can take place again. I've also included a few commented out lines of code for some of the scalar values required for tensorboard when I was randomly sampling the learning rate.

The next file is data_gen.py, within that file are a total of 3 different functions followed by a small workspace at the bottom.

-data_gen: this function builds out the training,label, and blank tensors by creating 30 different random conbinations of x,y pairs within specific ranges for each group for the training tensors. For the label tensors torch.zeros was initialized and for each index that corresponds with a 'red' sample the label tensor value was adjusted to 1. Next I generated all of the possible spots available on the plot to feed through the prediction function later.

-initial_graph: This function gave me a quick mock-up of what different distributions looked like when trying to find some ideal ranges when generating the training data.

-learning_rate_gen: When random sampling this function allows me to either randomly sample learning rates on a logarithmic scale and once a few models have been trained, narrow down on a custom range through the custom_range argument.

Below all three functions is the actual workspace itself. I begin by generating all of the data required then dividing everything by 100 because after training a few test models using input data that is super high tends to skew the weights and favor predictions closer to 1. Therefore the inputs needed to be normalized between 0 and 1. The next variable called layers_dims has the following format [# features, nodes in layer1, nodes in layer2, nodes in layer3]. This list is then passed into the network class where parameters are created in the correct shapes. The explicit for loop at the bottom was used to run the model iteratively when sampling for learning rates. Currently the learning_rate_gen function is set only to produce a single learning rate so the model should only run once if the script is built.